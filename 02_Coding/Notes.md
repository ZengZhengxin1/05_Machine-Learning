   PyTorch深度学习实践

PyTorch深度学习实践
=============

02 线性模型
-------

###  loss function

###   loss 指预测与原值差的平方的和

###   cost 指一组loss的相应的整体，可以做以下处理

###    MSE

###    MAE

03 梯度下降
-------

###  梯度下降属于优化的一种方法

###  用求导求出当前的点的梯度

###  随机梯度下降

###   拿部分样本的损失函数进行更新

###   引入随机性

###   但是不能进行并行计算

###    因此引入batch 的概念

04 反向传播
-------

###  要计算损失关于输入的倒数（从后往前），才能进行更新

###   但不同的神经元有不同的梯度

###   因此每一层可以从后往前追溯相应的梯度，就可以最终算出loss/cost 关于变量的梯度

###   最后就可以进行参数的更新

###  pytorch

###   数据类型 tensor

###    元素组成

###     data

###     grad

###      也是一个tensor类型

###  相关步骤

###   函数

###   loss

###    及构建计算图

###    l.backward()

###     计算后就会释放

05 pytorch 进行线性回归
-----------------

###  基本步骤

###   数据集准备

###   用类设计相关模型

###   构造损失函数及优化器

###   设置训练的周期

###  需要使用mini batch

###   重点转换为模型的构建

###   loss 要变成标量

###  function(\*args)

###   输入的函数的变量数量未知，会自动变为元组，\*kargs

06 逻辑回归
-------

###  图像的训练，可以使用CIFARCIFAR-10

###  饱和函数

###   超过阈值变化很小

###   可以将数值控制在0-1之间

###   即sigmoid函数

###  loss

###   KL散度

###   交叉熵

###    比较两个分布大小的方法

###   如果预测与实际的分布差别小，那么训练就成功了

07 多维数据输入
---------

###  进行降维操作

08 加载数据集
--------

###  batch 大计算速度快

###  epoch、batch、literation

###  数据是否打乱顺序 shuffle

###  加载数据的方法

###   一次性将所有数据存入内存中

###    即直接在class的init中直接加载

###   将文件名作为列表导入

09 多分类问题
--------

###  运用softmax进行数据的归一化

10 CNN
------

###  特征提取

###   使用kernel进行卷积操作

###   RGB

###    每层进行卷积

###   cat

###    将各卷积后的进行堆叠

###   padding

###    将图片拓展，保持输出不变

###   stride

###    步长

###     横移的长度

###   最大池化

11高级CNN
-------

###  GoogLeNet

###   inception module

###   1\*1 cov

###    进行数据融合

###  resnet
